# –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM

–≠—Ç–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –æ–ø–∏—Å—ã–≤–∞–µ—Ç **—É–ø—Ä–æ—â–µ–Ω–Ω—ã–π** –ø—Ä–æ—Ü–µ—Å—Å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ LLM –≤ –ø—Ä–æ–µ–∫—Ç `llm_clustering`.

> **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ –æ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–µ (–Ω–æ—è–±—Ä—å 2024):** –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –±—ã–ª–∞ —É–ø—Ä–æ—â–µ–Ω–∞:
> - –°–æ–∑–¥–∞–Ω `BaseLLMComponent` –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–¥–∞
> - –î–æ–±–∞–≤–ª–µ–Ω–∞ Pydantic-–≤–∞–ª–∏–¥–∞—Ü–∏—è LLM –æ—Ç–≤–µ—Ç–æ–≤
> - Settings —Ç–µ–ø–µ—Ä—å –∏–º–µ–µ—Ç –ø–ª–æ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É (–±–µ–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö config –∫–ª–∞—Å—Å–æ–≤)
> - `PipelineRunner` –æ–±—ä–µ–¥–∏–Ω–µ–Ω —Å `ClusteringPipeline`

## üéØ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç: SimpleLLMProvider

**–•–æ—Ä–æ—à–∞—è –Ω–æ–≤–æ—Å—Ç—å:** –¢–µ–ø–µ—Ä—å –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å–≤–æ–µ–≥–æ LLM –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å **–≤—Å–µ–≥–æ 1 –º–µ—Ç–æ–¥** - `chat_completion()`!

### –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä

```python
from llm_clustering import SimpleLLMProvider, ClusteringPipeline
import requests

class MyCustomLLM(SimpleLLMProvider):
    """–í–∞—à –∫–∞—Å—Ç–æ–º–Ω—ã–π LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä - –≤—Å–µ–≥–æ 1 –º–µ—Ç–æ–¥!"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.api_url = "https://api.your-llm.com"
        self.model = "your-model-name"
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        """–ï–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å."""
        response = requests.post(
            f"{self.api_url}/chat/completions",
            json={
                "model": self.model,
                "messages": messages,
                "temperature": temperature or 0.7,
                "max_tokens": max_tokens or 2000,
            },
            headers={"Authorization": f"Bearer {self.api_key}"}
        )
        return response.json()["choices"][0]["message"]["content"]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
llm = MyCustomLLM(api_key="your-api-key")
pipeline = ClusteringPipeline(llm_provider=llm)
result = pipeline.fit(df, text_column="text")
```

**–í–æ—Ç –∏ –≤—Å—ë!** SimpleLLMProvider –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç:
- ‚úÖ `describe_cluster()` - —á–µ—Ä–µ–∑ –≤–∞—à `chat_completion()`
- ‚úÖ `embed()` –∏ `cluster()` - –∫–∞–∫ `NotImplementedError` (–æ–Ω–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã)

---

## –û–±–∑–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### –î–≤–∞ –±–∞–∑–æ–≤—ã—Ö –∫–ª–∞—Å—Å–∞ –Ω–∞ –≤—ã–±–æ—Ä

#### 1. SimpleLLMProvider (‚≠ê –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
–î–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤ - –Ω—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ `chat_completion()`.

```python
from llm_clustering import SimpleLLMProvider

class MyLLM(SimpleLLMProvider):
    def chat_completion(self, messages, temperature=None, max_tokens=None) -> str:
        # –í–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        pass
```

**–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–ª—É—á–∞–µ—Ç–µ:**
- `describe_cluster()` - —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ `chat_completion()`
- `embed()` –∏ `cluster()` - –∑–∞–≥–ª—É—à–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã)

#### 2. BaseLLMProvider (–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π)
–ï—Å–ª–∏ –Ω—É–∂–µ–Ω –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –Ω–∞–¥ –≤—Å–µ–º–∏ –º–µ—Ç–æ–¥–∞–º–∏.

```python
from llm_clustering import BaseLLMProvider

class AdvancedLLM(BaseLLMProvider):
    def chat_completion(self, messages, ...) -> str: pass
    def describe_cluster(self, texts) -> str: pass
    def embed(self, texts) -> list[list[float]]: pass
    def cluster(self, texts, num_clusters) -> list[int]: pass
```

---

## –®–∞–≥ –∑–∞ —à–∞–≥–æ–º: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

### –í–∞—Ä–∏–∞–Ω—Ç A: –ü—Ä–æ—Å—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–±–µ–∑ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–±—Ä–∏–∫–µ)

–ï—Å–ª–∏ –≤–∞–º –Ω—É–∂–µ–Ω –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∞—à–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ - –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ–≥–æ –Ω–∞–ø—Ä—è–º—É—é:

```python
# my_llm.py
from llm_clustering import SimpleLLMProvider

class MyLLM(SimpleLLMProvider):
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        # –í–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        pass

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
from llm_clustering import ClusteringPipeline
from my_llm import MyLLM

pipeline = ClusteringPipeline(llm_provider=MyLLM(api_key="xxx"))
```

### –í–∞—Ä–∏–∞–Ω—Ç B: –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è (—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –≤ —Ñ–∞–±—Ä–∏–∫–µ)

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫—É:

#### 1. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

`src/llm_clustering/llm/my_provider.py`:

```python
"""My custom LLM provider."""

from llm_clustering.llm.simple_provider import SimpleLLMProvider
from llm_clustering.config import get_settings
from loguru import logger


class MyProvider(SimpleLLMProvider):
    """My custom LLM provider."""
    
    def __init__(self) -> None:
        """Initialize provider from settings."""
        settings = get_settings()
        self.api_key = settings.my_api_key
        self.api_url = settings.my_api_url or "https://api.example.com"
        self.model = settings.my_model or "default-model"
        
        if not self.api_key:
            raise ValueError("MY_API_KEY is required in .env file")
    
    def chat_completion(
        self,
        messages: list[dict[str, str]],
        temperature: float | None = None,
        max_tokens: int | None = None,
    ) -> str:
        """Chat completion implementation."""
        import requests
        
        try:
            response = requests.post(
                f"{self.api_url}/chat/completions",
                json={
                    "model": self.model,
                    "messages": messages,
                    "temperature": temperature or 0.7,
                    "max_tokens": max_tokens or 2000,
                },
                headers={"Authorization": f"Bearer {self.api_key}"},
                timeout=60
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except Exception as e:
            logger.error(f"API error: {e}")
            raise
```

#### 2. –î–æ–±–∞–≤—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏

`src/llm_clustering/config/settings.py`:

```python
class Settings(BaseSettings):
    # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ...
    
    # My Provider
    my_api_key: str = ""
    my_api_url: str = "https://api.example.com"
    my_model: str = "default-model"
```

`env.example`:

```bash
# My Provider
MY_API_KEY=your_api_key_here
MY_API_URL=https://api.example.com
MY_MODEL=default-model
```

#### 3. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ –≤ —Ñ–∞–±—Ä–∏–∫–µ

`src/llm_clustering/llm/factory.py`:

```python
from llm_clustering.llm.my_provider import MyProvider

class LLMFactory:
    _providers: dict[str, type[BaseLLMProvider]] = {
        "openai": OpenAIProvider,
        "anthropic": AnthropicProvider,
        "openrouter": OpenRouterProvider,
        "ollama": OllamaProvider,
        "my_provider": MyProvider,  # <-- –î–æ–±–∞–≤—å—Ç–µ –∑–¥–µ—Å—å
    }
```

#### 4. –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

`src/llm_clustering/llm/__init__.py`:

```python
from llm_clustering.llm.my_provider import MyProvider

__all__ = [
    # ...
    "MyProvider",
]
```

---

## –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

### –ü—Ä–∏–º–µ—Ä 1: REST API —Å requests

```python
from llm_clustering import SimpleLLMProvider
import requests
from loguru import logger


class RESTAPIProvider(SimpleLLMProvider):
    """Provider –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ REST API."""
    
    def __init__(self, api_key: str, base_url: str, model: str):
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        try:
            response = requests.post(
                f"{self.base_url}/v1/chat/completions",
                json={
                    "model": self.model,
                    "messages": messages,
                    "temperature": temperature or 0.7,
                    "max_tokens": max_tokens or 2000,
                },
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                },
                timeout=60
            )
            response.raise_for_status()
            return response.json()["choices"][0]["message"]["content"]
        except requests.RequestException as e:
            logger.error(f"API request failed: {e}")
            raise
```

### –ü—Ä–∏–º–µ—Ä 2: –õ–æ–∫–∞–ª—å–Ω—ã–π Ollama (–±–µ–∑ SDK)

```python
from llm_clustering import SimpleLLMProvider
import urllib3
import json


class LocalOllamaProvider(SimpleLLMProvider):
    """Provider –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ Ollama."""
    
    def __init__(self, model: str = "llama3", api_url: str = "http://localhost:11434"):
        self.model = model
        self.api_url = api_url
        self.http = urllib3.PoolManager()
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        payload = {
            "model": self.model,
            "messages": messages,
            "stream": False,
            "options": {
                "temperature": temperature or 0.7,
                "num_predict": max_tokens or 2000,
            }
        }
        
        response = self.http.request(
            "POST",
            f"{self.api_url}/api/chat",
            body=json.dumps(payload).encode("utf-8"),
            headers={"Content-Type": "application/json"}
        )
        
        return json.loads(response.data)["message"]["content"]
```

### –ü—Ä–∏–º–µ—Ä 3: OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π SDK

```python
from llm_clustering import SimpleLLMProvider
from openai import OpenAI  # –∏–ª–∏ –¥—Ä—É–≥–æ–π SDK


class OpenAICompatibleProvider(SimpleLLMProvider):
    """Provider –¥–ª—è OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö API."""
    
    def __init__(self, api_key: str, base_url: str, model: str):
        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature or 0.7,
            max_tokens=max_tokens or 2000,
        )
        return response.choices[0].message.content
```

### –ü—Ä–∏–º–µ—Ä 4: –ö–∞—Å—Ç–æ–º–∏–∑–∞—Ü–∏—è describe_cluster

–ï—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∏–∑–º–µ–Ω–∏—Ç—å —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –ø–æ–≤–µ–¥–µ–Ω–∏–µ `describe_cluster()`:

```python
from llm_clustering import SimpleLLMProvider


class CustomDescriptionProvider(SimpleLLMProvider):
    def __init__(self, api_key: str):
        self.api_key = api_key
    
    def chat_completion(self, messages, temperature=None, max_tokens=None):
        # –í–∞—à–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        pass
    
    def describe_cluster(self, texts: list[str]) -> str:
        """–ö–∞—Å—Ç–æ–º–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º."""
        prompt = f"""Analyze these customer requests and provide a brief summary:

{chr(10).join(f"- {text}" for text in texts[:15])}

Provide a brief 1-sentence summary of the main theme."""
        
        return self.chat_completion(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=100,
        )
```

---

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞

### –°–ø–æ—Å–æ–± 1: –ù–∞–ø—Ä—è–º—É—é

```python
from my_llm import MyCustomLLM
from llm_clustering import ClusteringPipeline

llm = MyCustomLLM(api_key="xxx")
pipeline = ClusteringPipeline(llm_provider=llm)
result = pipeline.fit(df, text_column="text")
```

### –°–ø–æ—Å–æ–± 2: –ß–µ—Ä–µ–∑ —Ñ–∞–±—Ä–∏–∫—É (–µ—Å–ª–∏ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω)

```python
from llm_clustering.llm.factory import get_llm_provider

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏
llm = get_llm_provider("my_provider")
```

### –°–ø–æ—Å–æ–± 3: –ß–µ—Ä–µ–∑ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ .env

```bash
DEFAULT_LLM_PROVIDER=my_provider
```

```python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç my_provider –∏–∑ .env
pipeline = ClusteringPipeline()
```

---

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π—Ç–µ —Ç–µ—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª –≤ `ai_experiments/`:

```python
"""Test custom LLM provider."""

from my_llm import MyCustomLLM
import pandas as pd


def test_provider():
    """–ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞."""
    
    # –¢–µ—Å—Ç chat_completion
    llm = MyCustomLLM(api_key="xxx")
    response = llm.chat_completion([
        {"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}
    ])
    print(f"Response: {response}")
    
    # –¢–µ—Å—Ç describe_cluster (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–µ—Ä–µ–∑ chat_completion)
    texts = [
        "–ù–µ –º–æ–≥—É –≤–æ–π—Ç–∏ –≤ –∞–∫–∫–∞—É–Ω—Ç",
        "–ó–∞–±—ã–ª –ø–∞—Ä–æ–ª—å",
        "–ü—Ä–æ–±–ª–µ–º–∞ —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π"
    ]
    description = llm.describe_cluster(texts)
    print(f"Cluster description: {description}")
    
    # –¢–µ—Å—Ç –≤ pipeline
    from llm_clustering import ClusteringPipeline
    df = pd.DataFrame({"text": texts})
    pipeline = ClusteringPipeline(llm_provider=llm)
    result = pipeline.fit(df, text_column="text")
    print(f"Clusters found: {len(result.clusters)}")


if __name__ == "__main__":
    test_provider()
```

---

## –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### ‚úÖ –•–æ—Ä–æ—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

1. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SimpleLLMProvider** –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ —Å–ª—É—á–∞–µ–≤
2. **–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–π—Ç–µ –æ—à–∏–±–∫–∏** –∏ –ª–æ–≥–∏—Ä—É–π—Ç–µ –∏—Ö —Å –ø–æ–º–æ—â—å—é `loguru.logger`
3. **–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π—Ç–µ —Ç–∞–π–º–∞—É—Ç—ã** –¥–ª—è HTTP-–∑–∞–ø—Ä–æ—Å–æ–≤ (–æ–±—ã—á–Ω–æ 60 —Å–µ–∫—É–Ω–¥)
4. **–í–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã** –≤ `__init__` (API –∫–ª—é—á–∏, URL)
5. **–î–æ–±–∞–≤–ª—è–π—Ç–µ docstrings** –¥–ª—è –º–µ—Ç–æ–¥–æ–≤ –∫–ª–∞—Å—Å–∞
6. **–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–∏–ø–∏–∑–∞—Ü–∏—é** –∏–∑ `typing`

### ‚ùå –ß–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏

1. **–ù–µ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–π—Ç–µ** `describe_cluster()` –±–µ–∑ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ - —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–ª–∏—á–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
2. **–ù–µ —Ä–µ–∞–ª–∏–∑—É–π—Ç–µ** `embed()` –∏ `cluster()` –µ—Å–ª–∏ –Ω–µ –Ω—É–∂–Ω–æ - –æ–Ω–∏ –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ
3. **–ù–µ –∑–∞–±—ã–≤–∞–π—Ç–µ** –ø—Ä–æ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ API
4. **–ù–µ —Ö—Ä–∞–Ω–∏—Ç–µ** —Å–µ–∫—Ä–µ—Ç—ã –≤ –∫–æ–¥–µ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ `.env`

---

## –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏

–ü–æ—Å–º–æ—Ç—Ä–∏—Ç–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ `src/llm_clustering/llm/`:

- **`simple_provider.py`** - –±–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å SimpleLLMProvider (‚≠ê –Ω–∞—á–Ω–∏—Ç–µ —Å –Ω–µ–≥–æ)
- **`openrouter_provider.py`** - REST API —Å requests
- **`ollama_provider.py`** - –ª–æ–∫–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å urllib3
- **`triton_provider.py`** - Triton Inference Server

---

## FAQ

### Q: –ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã?

**A:** –¢–æ–ª—å–∫–æ `chat_completion()` –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ `SimpleLLMProvider`.

### Q: –ù—É–∂–Ω—ã –ª–∏ embeddings?

**A:** –ù–µ—Ç, –º–µ—Ç–æ–¥ `embed()` –æ–ø—Ü–∏–æ–Ω–∞–ª–µ–Ω –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ –µ–≥–æ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç.

### Q: –ö–∞–∫ –∫–∞—Å—Ç–æ–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤?

**A:** –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç–µ `describe_cluster()` –≤ —Å–≤–æ—ë–º –∫–ª–∞—Å—Å–µ (—Å–º. –ü—Ä–∏–º–µ—Ä 4).

### Q: –ß—Ç–æ –¥–µ–ª–∞—Ç—å –µ—Å–ª–∏ –º–æ–π LLM API –Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å OpenAI?

**A:** –ü—Ä–æ—Å—Ç–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π—Ç–µ —Ñ–æ—Ä–º–∞—Ç –≤ `chat_completion()` - –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ `messages` –≤ –Ω—É–∂–Ω—ã–π –≤–∞–º —Ñ–æ—Ä–º–∞—Ç.

### Q: –ú–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ?

**A:** –î–∞, —Å–æ–∑–¥–∞–π—Ç–µ —Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç–∞–Ω—Å—ã –∏ –ø–µ—Ä–µ–¥–∞–≤–∞–π—Ç–µ –∏—Ö –≤ —Ä–∞–∑–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã.

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

–° `SimpleLLMProvider` –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–≤–æ–µ–≥–æ LLM —Å—Ç–∞–ª–æ **–≤ 4 —Ä–∞–∑–∞ –ø—Ä–æ—â–µ**:
- –†–∞–Ω—å—à–µ: 4 –∞–±—Å—Ç—Ä–∞–∫—Ç–Ω—ã—Ö –º–µ—Ç–æ–¥–∞
- –¢–µ–ø–µ—Ä—å: 1 –º–µ—Ç–æ–¥ (`chat_completion`)

–£–¥–∞—á–∏! üöÄ
